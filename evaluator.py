import json
import re
import os
from typing import List, Dict


class Evaluator:
    """
    Simple evaluator for spatial analysis tasks with direct comparison.
    """

    def __init__(self, output_dir="results", verbose=True):
        """
        Initialize the evaluator.

        Args:
            output_dir: Directory for saving results
            verbose: Whether to print detailed progress
        """
        self.output_dir = output_dir
        self.verbose = verbose

        # Create output directory
        os.makedirs(output_dir, exist_ok=True)

    def evaluate_response(self, generated_code: str, reference_answer: str) -> Dict[str, float]:
        """
        Evaluate a model's response against reference answer using direct comparison.

        Args:
            generated_code: Code generated by the model
            reference_answer: Reference answer (parcel IDs)

        Returns:
            Dictionary of evaluation metrics
        """
        # Extract Python code from generated response
        python_code = self.extract_python_code(generated_code)

        # Extract parcels from reference answer
        reference_parcels = self.parse_parcel_ids(reference_answer)

        # For direct comparison, we'll extract what would be the expected output of the code
        # We can check if the code includes print statements with parcel IDs
        expected_parcels = self.extract_expected_output_parcels(python_code)

        # Calculate accuracy metrics
        accuracy_metrics = self.calculate_accuracy_metrics(
            expected_parcels, reference_parcels)

        # Determine if code is likely to run
        code_correctness = self.check_code_correctness(python_code)

        # Combine metrics
        metrics = {
            **accuracy_metrics,
            "code_correctness": code_correctness
        }

        return metrics

    def extract_python_code(self, text: str) -> str:
        """Extract Python code from text that might include markdown."""
        if "```python" in text and "```" in text.split("```python", 1)[1]:
            return text.split("```python", 1)[1].split("```", 1)[0].strip()
        elif "```" in text and "```" in text.split("```", 1)[1]:
            return text.split("```", 1)[1].split("```", 1)[0].strip()
        else:
            return text.strip()

    def parse_parcel_ids(self, answer_text: str) -> List[str]:
        """Parse parcel IDs from an answer string."""
        # Check if the text is already a list (from JSON)
        if answer_text.startswith('[') and answer_text.endswith(']'):
            try:
                # Try to parse as JSON
                ids = json.loads(answer_text)
                if isinstance(ids, list):
                    return [str(id).strip() for id in ids if id]
            except:
                pass

        # Extract with regex
        pattern = r"'([0-9A-Za-z\.-]+)'"
        matches = re.findall(pattern, answer_text)
        if matches:
            return matches

        # Split by commas if no matches found
        if ',' in answer_text:
            return [id.strip().strip("'\"") for id in answer_text.split(',')]

        return []

    def extract_expected_output_parcels(self, code: str) -> List[str]:
        """
        Extract expected parcel IDs from the code output.

        Args:
            code: Python code to analyze

        Returns:
            List of parcel IDs that would be output
        """
        # For direct comparison, we'll just extract any IDs that appear in the code
        # This simulates what would be returned by the code without executing it

        # Look for print statements with result_ids or similar variables
        result_pattern = r"print\(.*(?:result_ids|parcel_ids|ids).*\)"
        result_matches = re.findall(result_pattern, code, re.IGNORECASE)

        if result_matches:
            # Check if there are any hard-coded IDs in the print statements
            for match in result_matches:
                id_pattern = r"'([0-9A-Za-z\.-]+)'"
                ids = re.findall(id_pattern, match)
                if ids:
                    return ids

        # If we can't find explicit IDs, just return an empty list
        # In a real implementation, you would execute the code and get actual results
        return []

    def check_code_correctness(self, code: str) -> float:
        """
        Check if code is likely to run correctly.

        Args:
            code: Python code to check

        Returns:
            Binary score (0/1) indicating correctness
        """
        # Check key components that are necessary for the code to work
        has_imports = "import" in code and (
            "geopandas" in code or "gpd" in code)
        has_data_loading = "read_file" in code
        has_filtering = any(term in code for term in [
                            "filter", "[", "]", "query", "where"])
        has_print = "print" in code

        # Code is considered correct if it has all essential components
        return 1.0 if (has_imports and has_data_loading and has_filtering and has_print) else 0.0

    def calculate_accuracy_metrics(self, generated_parcels: List[str],
                                   reference_parcels: List[str]) -> Dict[str, float]:
        """
        Calculate accuracy metrics for parcel IDs.

        Args:
            generated_parcels: Parcels from the generated code
            reference_parcels: Parcels from the reference answer

        Returns:
            Dictionary with result_match_rate, precision, recall, and f1_score
        """
        if not generated_parcels or not reference_parcels:
            return {
                "result_match_rate": 0.0,
                "precision": 0.0,
                "recall": 0.0,
                "f1_score": 0.0
            }

        # Convert to sets for intersection/union operations
        generated_set = set(generated_parcels)
        reference_set = set(reference_parcels)

        # Calculate true positives (correctly identified parcels)
        true_positives = len(generated_set.intersection(reference_set))

        # Calculate metrics
        precision = true_positives / \
            len(generated_set) if generated_set else 0.0
        recall = true_positives / len(reference_set) if reference_set else 0.0
        f1_score = 2 * precision * recall / \
            (precision + recall) if precision + recall > 0 else 0.0

        # Result match rate is the percentage of matching parcels relative to the total unique parcels
        result_match_rate = true_positives / \
            len(generated_set.union(reference_set))

        return {
            "result_match_rate": result_match_rate,
            "precision": precision,
            "recall": recall,
            "f1_score": f1_score
        }

    def compute_average_metrics(self, results: List[Dict]) -> Dict[str, float]:
        """
        Compute average metrics across all samples (no categorization).

        Args:
            results: List of evaluation results

        Returns:
            Dictionary of averaged metrics
        """
        metrics = [
            "result_match_rate", "precision", "recall", "f1_score",
            "code_correctness", "response_time", "tokens_used"
        ]

        # Initialize sums
        sums = {metric: 0.0 for metric in metrics}

        # Sum up metrics
        for result in results:
            for metric in metrics:
                if metric in result["evaluation"]:
                    sums[metric] += result["evaluation"][metric]

        # Calculate averages
        return {metric: sums[metric] / len(results) if results else 0.0 for metric in metrics}

    def save_results(self, method: str, results: List[Dict], summary: Dict) -> None:
        """
        Save evaluation results to file.

        Args:
            method: Evaluation method (no_shot, few_shot, fine_tuned)
            results: List of evaluation results
            summary: Summary metrics
        """
        # Create results directory if it doesn't exist
        os.makedirs(self.output_dir, exist_ok=True)

        # Save full results
        results_path = os.path.join(self.output_dir, f"{method}_results.json")
        with open(results_path, 'w') as f:
            json.dump(results, indent=2, fp=f)

        # Save summary
        summary_path = os.path.join(self.output_dir, f"{method}_summary.json")
        with open(summary_path, 'w') as f:
            json.dump(summary, indent=2, fp=f)

        if self.verbose:
            print(f"Results saved to {results_path}")
            print(f"Summary saved to {summary_path}")
            print(f"Average F1 Score: {summary.get('f1_score', 0.0):.4f}")
            print(
                f"Average Code Correctness: {summary.get('code_correctness', 0.0):.4f}")
